\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}

\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage[title]{appendix}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{manyfoot}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{url}

\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}

\begin{document}

\title[Neuromorphic Spiking Trees]{Neuromorphic Spiking Trees: Energy-Efficient Decision Trees via Latency-Aware Entropy Induction}

\author[1,2]{\fnm{Haythem} \sur{Ghazouani}}\email{haythem.ghazouani@enicar.u-carthage.tn}

\affil[1]{\orgdiv{Universit\'{e} de Tunis El Manar}, \orgname{Institut Sup\'{e}rieur d'Informatique, Research Team on Intelligent Systems in Imaging and Artificial Vision (SIIVA), LR16ES06 Laboratoire de recherche en Informatique, Mod\'{e}lisation et Traitement de l'Information et de la Connaissance (LIMTIC)}, \orgaddress{\street{2 Rue Abou Rayhane Bayrouni}, \city{Ariana}, \postcode{2080}, \country{Tunisia}}}

\affil[2]{\orgdiv{Universit\'{e} de Carthage}, \orgname{Ecole Nationale d'Ing\'{e}nieurs de Carthage}, \orgaddress{\street{45 Rue des Entrepreneurs}, \city{Tunis-Carthage}, \postcode{2035},  \country{Tunisia}}}

\abstract{
The burgeoning field of edge intelligence is currently constrained by a fundamental trade-off between the high performance of deep learning models and the simultaneous requirements for hardware efficiency and decision transparency. This paper introduces the \textit{Neuromorphic Spiking Tree} (NST), a novel architectural class that bridges symbolic decision-making with event-driven neuromorphic computation. By reformulating decision tree nodes as temporal comparators operating under a Time-to-First-Spike (TTFS) encoding paradigm, we establish a mathematically rigorous framework for co-optimizing classification accuracy and inference latency through latency-aware entropy induction. Unlike conventional spiking neural networks that rely on complex membrane dynamics, the NST preserves the direct interpretability of decision paths while inheriting the extreme activation sparsity of neuromorphic substrates. Extensive empirical evaluation across tabular, sensor-based, and image benchmarks demonstrates that the NST and its ensemble counterpart, the \textit{Neuromorphic Spiking Forest} (NSF), achieve competitive accuracy (e.g., 94.7\% on Breast Cancer and 89.0\% on MNIST). We quantify efficiency through a hardware-equivalent computational footprint analysis based on explicit Synaptic Operation (SOP) counts, calibrated to measured pJ-scale spiking costs in neuromorphic hardware. Results show up to 40$\times$ reduction in hardware-equivalent footprints compared to standard artificial neural network baselines, with model decisions resolved in as few as 7 to 51 time-steps. This work establishes the symbolic-neuromorphic paradigm as a robust, interpretable, and hardware-efficient alternative for mission-critical edge deployment and Green AI applications. The complete implementation and reproducibility package are available at \url{https://github.com/haythemghz/Neuromorphic_Spiking_Trees}.
}

\keywords{Neuromorphic Computing, Spiking Neural Networks, Decision Trees, Latency-Aware Optimization, Green AI, Interpretability}

\maketitle

\section{Introduction}

The ubiquitous deployment of artificial intelligence in Internet-of-Things (IoT) ecosystems and autonomous systems has exposed a critical divergence: the models achieving state-of-the-art accuracy are increasingly incompatible with the stringent power and transparency mandates of edge environments \cite{strubell2019energy, yan2024reconsidering}. Deep Neural Networks (DNNs), while powerful, function as "black-box" estimators whose internal logic resists human audit, posing significant risks in safety-critical domains such as medical diagnostics and autonomous navigation. Simultaneously, the computational density of synchronous matrix operations in conventional architectures strains the thermal and battery budgets of embedded devices, necessitating a fundamental shift toward "Green AI" paradigms that prioritize energy efficiency alongside predictive accuracy \cite{schwartz2020green, lacoste2019quantifying}. In this context, we propose the Neuromorphic Spiking Tree not as a universal replacement for deep learning, but as a specialized, ultra-low-power alternative for resource-constrained inference tasks where auditability is paramount.

Neuromorphic computing offers a transformative path forward by mimicking the event-driven, asynchronous processing of biological brains \cite{roy2019towards, maass1997networks}. Beyond classification, recent work in natural language processing \cite{knipper2024snnlp} shows that spiking encodings can yield substantial energy gains compared to traditional neural networks, underscoring the versatility of spiking systems. Spiking Neural Networks (SNNs), the third generation of neural models, encode information in the precise timing of discrete action potentials, enabling hardware-level sparsity where energy is consumed only upon signal transmission. Specialized manycore processors, such as Intel’s Loihi \cite{Davies2018}, have demonstrated significant leads in energy-delay product for temporal tasks. However, the adoption of SNNs is hindered by the non-differentiability of spike functions—which complicates gradient-based training—and more crucially, by their inherent lack of interpretability. Despite advancements in spike-based attribution, deep spiking architectures remain as opaque as their analog counterparts.

Symbolic models, particularly Decision Trees \cite{Breiman1984} and their modern ensemble derivatives (Random Forests, Gradient Boosted Trees) \cite{chen2016xgboost}, occupy the opposite end of the spectrum. They provide hierarchical, axis-aligned decision boundaries that are directly inspectable and robust to feature scales. Yet, traditional decision trees are inherently "atemporal"; they process static feature vectors in a single shot, failing to exploit the temporal sparsity and low-latency potential of neuromorphic substrates.

In this work, we bridge this gap by introducing the \textbf{Neuromorphic Spiking Tree (NST)}, an architecture that establishes a new class of symbolic-neuromorphic models. By mapping continuous input features to Time-to-First-Spike (TTFS) latencies, we reformulate the decision node as a temporal comparator that resolves classification as a race between feature arrival times. \textbf{Crucially, the NST is the first symbolic model whose inference cost is natively data-dependent in simulation time, not just tree depth.} Unlike previous hybrid attempts, we propose a direct, deterministic induction algorithm based on \textit{Latency-Aware Entropy Incorporation}, allowing for the explicit co-optimization of purity and inference time. This approach yields models that are not only interpretable and hardware-amenable but also statistically optimized for the Accuracy-Latency Pareto frontier.

The contributions of this paper are organized as follows: (1) we formalize the Neuromorphic Spiking Tree as a temporal decision operator with proven equivalence to classical induction in the limit; (2) we introduce a latency-aware entropy metric that penalizes late-arriving features to minimize inference time; (3) we present the Neuromorphic Spiking Forest (NSF), an ensemble extension that leverages temporal consensus for robust early-exit behavior; and (4) we provide an extensive benchmark against modern classical baselines (including XGBoost and LightGBM) and analyze hardware-equivalent computational footprints using measured pJ-scale spiking costs. Our results demonstrate that the NST/NSF paradigm offers a disciplined approach to efficient, interpretable AI, providing a scalable solution for the next generation of resource-constrained intelligent systems.

\section{Related Work}
\label{sec:related_work}

The convergence of symbolic logic and spiking computation sits at the intersection of several rapidly evolving fields. Standardization efforts like MLPerf Power 2.0 \cite{Reddi2020} are now essential for quantifying these gains fairly across heterogeneous hardware. Recent comparative reviews \cite{Ferreira2025} further highlight the nuanced trade-offs between conventional deep networks and spiking implementations, particularly for edge systems were energy and latency constraints dominate design choices. In addition to latency-aware inference, large-scale spiking architectures such as SpikeRL \cite{Tahmid2025} demonstrate significant energy and performance gains in continuous control, emphasizing the generality of SNN energy advantages beyond classification tasks. Efficient training strategies and evaluation frameworks for SNNs have been consolidated in recent surveys \cite{rast2024efficient}, providing best-practice guidelines on energy-accuracy trade-offs.

\subsection{Energy-Efficient Neuromorphic Learning}
Traditional SNN evaluations often overclaim efficiency by ignoring memory access and data movement overheads. Recent critiques by Yan et al. \cite{yan2024reconsidering} suggest that SNNs must maintain high activation sparsity to truly outperform optimized ANNs. Hybrid hardware-software approaches have emerged to address this, co-optimizing sparsity and architectural constraints for edge deployment \cite{Nowshin2024}. For visual and sensor-based tasks, brain-inspired multi-scale detectors have demonstrated that direct training of spiking convolutional neurons can achieve high accuracy with minimal energy footprints \cite{li2023brain}.

\subsection{Spiking Hybrid Models and Sequential Modeling}
The integration of SNNs with traditional machine learning techniques is a nascent but promising area. Zarzoor et al. \cite{zarzoor2023intrusion} proposed an IoT intrusion detection method (IDS-SNNDT) that uses decision trees to select optimal samples for spiking neurons. Other works have explored temporal reversible SNNs \cite{hu2024high} to reduce training memory. Sequential modeling has also been enhanced via Central Pattern Generators \cite{lv2024advancing}, showing the breadth of temporal coding applications. Hybrid temporal decision trees have also been explored in time-series contexts \cite{pagliarini2024neural}, indicating the potential of blending neural representations with formal symbolic inference for dynamic data. Adaptive ensemble strategies with distillation \cite{konstantaropoulos2025dynamic} have been shown to maintain high accuracy while substantially reducing energy costs, suggesting ensemble paradigms are a complementary direction for energy-aware spiking models.

\subsection{Interpretability and Explainability}
As SNNs scale, explaining their internal representations becomes critical, especially in medical and security domains. Bitar et al. \cite{Bitar2023} adapted gradient-based attribution methods (SNN-Grad3D) to identify task-critical spikes. Concurrently, research into interpretable decision trees continues with focus on feature learning \cite{Good2023}. The NST architecture uniquely synthesizes these paths, ensuring that every classification follows a unique, auditable logical path in the temporal domain, offering a competitive alternative to black-box spiking networks \cite{Ren2024}.

\section{Methodology}
\label{sec:methodology}

This section establishes the formal framework underlying the Neuromorphic Spiking Tree and its ensemble extension. We begin by defining the notation and problem formulation, proceed through the temporal encoding scheme and node-level decision logic, and conclude with the deterministic induction procedure and ensemble aggregation mechanism. Throughout, we emphasize the design constraints that preserve both neuromorphic compatibility and decision traceability.

\subsection{Biological Motivation and TTFS Encoding}
The NST architecture is grounded in the principle of \textit{Time-to-First-Spike} (TTFS) coding \cite{comsa2020temporal}, which posits that the most discriminative information is often contained in the timing of the very first action potential following stimulus onset. This encoding scheme is highly efficient as it minimizes synaptic activity (one spike per neuron) and enables rapid inference.

\subsection{Notation and Formalism}
\label{subsec:notation}

The symbols employed throughout this paper are summarized in Table~\ref{tab:notation} for reference.

\begin{table}[h]
\centering
\caption{Summary of Formal Notation}
\label{tab:notation}
\begin{tabular}{ll}
\hline
\textbf{Symbol} & \textbf{Definition} \\ \hline
$\mathbf{x} \in \mathbb{R}^D$ & Input feature vector of dimension $D$. \\
$y \in \{1, \dots, C\}$ & Class label for $C$-class classification. \\
$\mathcal{T}$ & A Neuromorphic Spiking Tree (NST). \\
$T_{max}$ & Total temporal simulation window (e.g., 50 steps). \\
$t_{\mathbf{x}, i}$ & Spike time of feature $i$ under temporal encoding. \\
$n_k$ & A specific node in the tree indexed by $k$. \\
$\theta_k \in [0, T_{max}]$ & Temporal split threshold of node $n_k$. \\
$f_k \in \{1, \dots, D\}$ & Feature index assigned to node $n_k$. \\
$\lambda$ & Regularization parameter for accuracy-latency trade-off. \\
$\mathcal{F}$ & A Neuromorphic Spiking Forest (NSF) of $N$ trees. \\ \hline
\end{tabular}
\end{table}

\subsection{Problem Formulation}
\label{subsec:problem_formulation}

We address the supervised classification problem in which a model $\mathcal{M}$ maps an input vector $\mathbf{x}$ to a predicted class $\hat{y}$. In the neuromorphic setting, we extend this standard formulation to a multi-objective optimization problem that explicitly accounts for inference latency. Let $\mathcal{A}(\mathcal{M})$ denote the classification accuracy achieved by model $\mathcal{M}$ on a held-out validation set, and let $\mathcal{L}(\mathcal{M}, \mathbf{x})$ denote the inference latency, measured as the number of simulation time-steps required to produce a classification for sample $\mathbf{x}$. The objective is to identify a model $\mathcal{M}^*$ that minimizes a composite cost function balancing error rate and temporal cost:
\begin{equation}
J(\mathcal{M}) = (1 - \mathcal{A}(\mathcal{M})) + \lambda \cdot \mathbb{E}_{\mathbf{x} \sim P(\mathbf{x})} [\mathcal{L}(\mathcal{M}, \mathbf{x})]
\label{eq:objective}
\end{equation}
The regularization parameter $\lambda$ governs the trade-off between predictive power and computational speed: larger values of $\lambda$ penalize slow inference more heavily, inducing shallower and faster decision structures at the potential expense of accuracy. Crucially, the model must operate under event-driven constraints wherein computation occurs only upon the arrival of discrete spike events, precluding the dense synchronous operations characteristic of conventional neural architectures.

\subsection{Temporal Feature Encoding}
\label{subsec:temporal_encoding}

The transition from magnitude-based to temporal decision-making requires a principled encoding of numerical features into the spike domain. We adopt Time-to-First-Spike (TTFS) encoding, a scheme that represents each feature value as the latency of a single spike relative to an initial reference time. Given a feature $x_i$ normalized to the interval $[x_{min}, x_{max}]$, the corresponding spike time $t_{\mathbf{x}, i}$ is computed as:
\begin{equation}
t_{\mathbf{x}, i} = T_{max} \cdot \left( 1 - \frac{x_i - x_{min}}{x_{max} - x_{min}} \right)
\label{eq:ttfs}
\end{equation}
This linear mapping assigns earlier spike times to larger feature values, reflecting the biological principle that stronger stimuli elicit faster neural responses. The encoding preserves the ordinal relationships among features while confining all spike events to a bounded temporal window $[0, T_{max}]$. Unlike rate coding, which represents information through spike counts accumulated over extended intervals, TTFS encoding requires at most one spike per input dimension, minimizing synaptic activity and enabling rapid inference that terminates as soon as the relevant decision path is resolved.

\subsection{Biomorphic Feature Extraction}
\label{subsec:biomorphic}

While tabular datasets permit direct TTFS encoding, unstructured image data (e.g., MNIST, CIFAR-10) contains dense spatial correlations that a simple axis-aligned tree struggle to capture. To address this, we introduce a \textit{biomorphic feature extraction} stage that precedes the TTFS encoding. Inspired by the early visual cortex, we apply a bank of $K$ fixed-weight convolutional kernels $\mathcal{K}_j$, followed by local pooling. The activation $a_{j,u,v}$ for an image patch is computed as:
\begin{equation}
a_{j,u,v} = \sigma \left( \sum_{m,n} \mathcal{K}_j(m,n) \cdot I(u+m, v+n) \right)
\label{eq:conv}
\end{equation}
where $I$ is the input image and $\sigma$ is a non-linear activation (e.g., ReLU). These continuous-valued activations are then mapped to spike latencies via Eq.~(\ref{eq:ttfs}). This hybrid approach ensures that the "spikes" entering the NST carry high-level structural information, effectively using a shallow neuromorphic backbone to enhance the representational capacity of the subsequent symbolic tree.

\subsection{Spiking Node as a Temporal Decision Operator}
\label{subsec:node_operator}

Each internal node $n_k$ in a Neuromorphic Spiking Tree functions as a temporal comparator defined by a feature index $f_k$ and a temporal threshold $\theta_k$. The node monitors the arrival time of the spike corresponding to feature $f_k$ and routes the signal to one of two child branches based on a simple temporal test. If the spike arrives before the threshold, the node fires an excitatory signal to its right child, corresponding to the high-value branch in conventional terminology. If the temporal window elapses without an early spike, the node defaults to its left child at time $\theta_k$. Formally, the branch selection $B$ and exit latency $t_{out}$ are governed by:
\begin{equation}
(B, t_{out}) = \begin{cases} 
(\text{Right}, t_{\mathbf{x}, f_k} + \delta) & \text{if } t_{\mathbf{x}, f_k} < \theta_k \\
(\text{Left}, \theta_k + \delta) & \text{otherwise}
\end{cases}
\label{eq:node_logic}
\end{equation}
where $\delta$ represents a minimal synaptic propagation delay. This temporal gating mechanism translates directly to neuromorphic hardware primitives while preserving semantic equivalence to classical threshold tests.
\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{nst_mechanism_concept.png}
    \caption{Conceptual overview of the Neuromorphic Spiking Tree mechanism. \textbf{(a)} High-magnitude features are encoded as early spikes (TTFS). \textbf{(b)} Each node acts as a race condition: if the relevant feature spike arrives before the temporal threshold $\theta_k$, the decision is routed immediately to the right child (early exit). Otherwise, the node necessitates a wait until $t=\theta_k$ to default to the left child.}
    \label{fig:nst_mechanism}
\end{figure}

\begin{proposition}
A spiking decision node operating under TTFS encoding is functionally equivalent to a threshold test $x_i > \alpha$ in classical decision trees.
\end{proposition}
\textit{Proof.} Since $t_{\mathbf{x}, i}$ is a monotonically decreasing function of $x_i$ under the TTFS mapping, the condition $t_{\mathbf{x}, i} < \theta_k$ holds if and only if the normalized feature value $\hat{x}_i$ exceeds $1 - \theta_k/T_{max}$. Thus, the temporal threshold $\theta_k$ maps bijectively to a magnitude threshold $\alpha = 1 - \theta_k/T_{max}$, establishing functional equivalence.

\subsection{Neuromorphic Spiking Tree Architecture}
\label{subsec:nst_arch}

A Neuromorphic Spiking Tree $\mathcal{T}$ is a directed acyclic graph in which each internal node implements the temporal decision operation defined above and each leaf node stores a class prediction. Inference proceeds as a temporal race: an input sample $\mathbf{x}$ initiates spike events across all input dimensions according to the TTFS encoding, and these spikes propagate through the tree structure with each node resolving its branch decision at the moment sufficient temporal information becomes available. The total inference latency $L(\mathcal{T}, \mathbf{x})$ equals the cumulative exit time along the traversed path, which depends not only on tree depth but also on the specific spike times encountered. This data-dependent latency distinguishes NST from conventional trees, where inference cost is determined solely by depth: an NST can terminate rapidly when early-arriving features suffice for classification, even if the tree structure is nominally deep.

The preservation of decision traceability follows directly from the tree structure. Unlike deep spiking networks where predictions emerge from distributed membrane dynamics, every classification produced by an NST corresponds to a unique root-to-leaf path. Each node along this path contributes an interpretable temporal justification: the decision at node $n_k$ reflects whether feature $f_k$ exhibited a strong value (early spike, right branch) or a weak value (late or absent spike, left branch). This explicit decision log enables domain experts to audit classifications, identify influential features, and detect failure modes arising from distributional shift or adversarial perturbation.

\subsection{Latency-Aware Entropy Induction}
\label{subsec:entropy_induction}

To construct the Neuromorphic Spiking Tree, we employ a deterministic, top-down induction algorithm inspired by classical decision tree learning (CART/ID3), but fundamentally reformulated to operate within the constraints of the temporal encoding. Unlike traditional methods that solely maximize purity (Information Gain), our induction process explicitly balances predictive power against inference latency at every split.

We define the quality of a potential split $(f, \theta)$ at node $\mathcal{N}$ by a composite objective function that penalizes splits heavily reliant on late-arriving spikes. Let $S$ be the set of samples at node $\mathcal{N}$, and $S_L, S_R$ be the partitions resulting from the split. The purely informational component is given by the standard Information Gain:
\begin{equation}
\Delta H(S, f, \theta) = H(S) - \left( \frac{|S_L|}{|S|} H(S_L) + \frac{|S_R|}{|S|} H(S_R) \right)
\label{eq:info_gain}
\end{equation}
where $H(\cdot)$ denotes Shannon entropy. To incorporate latency awareness, we introduce a temporal cost term. Recall from Eq.~(\ref{eq:ttfs}) that a split on feature $x_{i,f}$ with threshold $\alpha$ corresponds to a temporal threshold $\theta = T_{max}(1 - \alpha)$. Samples satisfying $t_{x,f} < \theta$ (strong inputs) traverse the right branch immediately upon spike arrival, while samples with $t_{x,f} \ge \theta$ (weak/absent inputs) default to the left branch at time $\theta$. The expected local inference latency for this split is:
\begin{equation}
E[t_{out}] = \frac{|S_R|}{|S|} E[t_{x,f} \mid S_R] + \frac{|S_L|}{|S|} (\theta + \delta)
\label{eq:expected_latency}
\end{equation}
where $\delta$ represents the synaptic delay. The unified split score is then defined as:
\begin{equation}
Score(f, \theta) = \Delta H(S, f, \theta) - \lambda \cdot \frac{E[t_{out}]}{T_{max}}
\label{eq:split_score}
\end{equation}
The hyperparameter $\lambda$ controls the aggressiveness of the latency optimization. When $\lambda=0$, the algorithm recovers standard CART behavior; as $\lambda$ increases, the induction process favors "early" features—those where discriminative spikes arrive rapidly—even if they offer slightly lower immediate purity improvement. This creates a natural bias towards features that can be resolved quickly, placing them near the root of the tree. The tree is built recursively until a maximum depth is reached or node purity exceeds a threshold, yielding a structure that is inherently optimized for the Time-to-First-Spike encoding.

% Sections 3.6 and 3.7 removed/moved to Appendix for narrative focus.

\subsection{Neuromorphic Spiking Forest}
\label{subsec:nsf}

While individual Neuromorphic Spiking Trees offer maximal interpretability, their representational capacity is limited by the expressiveness of axis-aligned temporal splits. To address complex classification tasks that benefit from model aggregation, we introduce the Neuromorphic Spiking Forest: a lightweight ensemble of $N \leq 10$ trees constructed via bootstrap aggregating (bagging) and random subspace projection. The constraint on ensemble size reflects the interpretability-capacity trade-off: larger ensembles improve accuracy but dilute the explanatory power of individual trees. Unlike classical random forests that aggregate predictions through magnitude-based voting, the NSF must resolve conflicts in the temporal domain where trees may reach decisions at different times.

The design of the NSF promotes diversity through two stochastic mechanisms. First, each tree is trained on a bootstrap sample of the original dataset, ensuring exposure to different instance distributions. Second, at each node split, the algorithm considers only a random subset of features (typically $\sqrt{D}$), forcing the trees to decorrelate their latent decision paths. This structural diversity compensates for the greedy nature of the induction algorithm, effectively reducing the ensemble's variance while the latency-aware entropy criterion minimizes bias. The ensemble integrates these diverse perspectives through a temporal consensus mechanism that balances decision quality against response time.

\subsection{Temporal Voting and Consensus}
\label{subsec:temporal_voting}

Aggregation within the Neuromorphic Spiking Forest employs a temporal consensus rule. Each constituent tree $\mathcal{T}_j$ produces a prediction $\hat{y}^j$ with associated latency $L^j$. The forest prediction $\hat{Y}$ is determined by majority vote among trees whose inference completes before a consensus deadline:
\begin{equation}
\hat{Y} = \text{mode}(\{\hat{y}^j \mid L^j \leq T_{consensus}\})
\label{eq:consensus}
\end{equation}
This mechanism enables early-exit behavior: if a majority of trees reach agreement before the deadline, the forest terminates inference immediately without waiting for slower members. The consensus threshold $T_{consensus}$ provides a tunable parameter controlling the latency-accuracy trade-off at the ensemble level, independent of the per-tree regularization parameter $\lambda$. In practice, this temporal voting scheme often reduces effective ensemble latency well below the maximum individual tree latency, as the fastest-agreeing subset drives the decision.

% Section removed: Error-Focused Evolution is gone.

\subsection{Dendritic Integration and Multi-Feature Splits}
\label{subsec:dendritic}

The representational capacity of standard NST nodes is limited by the expressiveness of axis-aligned temporal splits. To capture non-linear feature interactions without increasing tree depth, we incorporate \textit{dendritic integration} logic at the node level. Inspired by the computational properties of memristive neurons \cite{li2023emerging}, we permit compound splits that process a linear combination of spike latencies. A dendritic node $n_k$ resolves its branch decision based on a weighted sum of inputs $\mathbf{w}_k \cdot \mathbf{t}_{\mathbf{x}, k}$, where $\mathbf{t}_{\mathbf{x}, k}$ is a subset of feature latencies.

The firing condition for the right branch is generalized to:
\begin{equation}
\sum_{i \in \mathcal{F}_k} w_{k,i} \cdot t_{\mathbf{x}, i} < \theta_k
\label{eq:dendritic_split}
\end{equation}
where $\mathcal{F}_k$ defines the set of interacting features at node $k$. This formulation allows the tree to learn "oblique" decision boundaries in the temporal domain. Empirically, we constrain $|\mathcal{F}_k| \leq 2$ to maintain interpretability while addressing the capacity limits observed in complex datasets such as UCI Wine. By mapping multiple synapses to a single decision threshold, the NST mimics biological dendritic trees that perform local non-linear integration before somatic firing.

\subsection{Robustness via Stochastic Encoding}
\label{subsec:robustness_noise}

Real-world neuromorphic sensors and hardware substrates exhibit inherent stochasticity in spike generation and propagation. To ensure the NST remains robust to such variability, we analyze the impact of \textit{stochastic jitter} during the induction phase. By training under noisy conditions, the latency-aware entropy algorithm learns to prioritize features whose discriminative power is resilient to temporal shifts. This robustness analysis is critical for safety-critical edge applications where environmental noise could otherwise trigger erroneous decision paths in purely deterministic models.

\subsection{Hardware-Equivalent Computational Footprint Analysis}
\label{subsec:heav_v2}

A central claim of neuromorphic architectures is their superior computational efficiency compared to von Neumann implementations. To quantify this for the NST in a hardware-agnostic but scientifically rigorous manner, we adopt a framework based on \textit{Hardware-Equivalent Computational Footprints}. Throughout this section, efficiency is analyzed in terms of these hardware-equivalent footprints derived from explicit operation counts; no absolute on-chip energy measurements are claimed. This approach replaces speculative energy estimates with counts of Synaptic Operations (SOPs) and active neurons, which are the primary contributors to dynamic energy consumption in neuromorphic substrates.

We define a Synaptic Operation (SOP) as a single node-level temporal comparison occurring during the inference path. The hardware-equivalent energy complexity $E_{eq}$ is modeled as being proportional to the total operation count:
\begin{equation}
E_{eq} \propto N_{SOP} + \alpha \cdot N_{neuron}
\label{eq:heav_model}
\end{equation}
where $N_{SOP}$ is the number of decision nodes traversed, $N_{neuron}$ is the number of active structural neurons, and $\alpha$ is a relative weighting factor representing the cost of a neuron update relative to a synaptic comparison. Static and leakage power are not considered, as the analysis focuses on relative dynamic computational costs and all methods are assumed to operate on identical hardware. This approach ensures that efficiency gains reflect the inherent sparse, event-driven nature of the symbolic-neuromorphic induction process rather than specific hardware idling characteristics.

Based on empirical measurements reported for the Loihi 2 neuromorphic processor \cite{davies2021advancing}, synaptic operations (SOPs) and neuron updates incur approximately **10.12 pJ** per event. Using this calibration, we translate our measured SOP counts into hardware-equivalent energy proxies. For instance, the NST's average footprint of 53.3 SOPs corresponds to an estimated dynamic energy of **0.54 nJ per inference**, whereas a matched spiking MLP baseline (30-128-2) consumes approximately **41.45 nJ**. We therefore interpret these calibrated SOP counts as verifiable hardware-equivalent energy metrics, without claiming absolute on-chip consumption for specific unreleased hardware. {\textbf{Fairness Statement: To ensure an equitable comparison across all spiking models, the input encoding (TTFS), simulation window ($T_{max}$), and temporal thresholding resolution are matched exactly between the NST and the baseline SNN models.}} Since the NST terminates inference as soon as a leaf is reached, its computational footprint is natively adaptive to input difficulty—simple samples are resolved by short, early-firing paths with minimal SOP counts, while ambiguous cases consume more resources. 


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/heav_v2_efficiency.png}
    \caption{Hardware-Equivalent Computational Footprint Comparison. The NST achieves an order-of-magnitude reduction in Synaptic Operations (SOPs) compared to a standard SNN baseline (30-128-2 MLP) under equivalent TTFS encoding and simulation parameters.}
    \label{fig:efficiency}
\end{figure}

\subsection{Theoretical Analysis}
\label{subsec:theory}

In this section, we formally characterize the properties of the proposed NST algorithm, establishing its relationship to classical decision trees and deriving bounds on its temporal behavior.

\begin{proposition}[Equivalence to Classical Induction]
Let $\mathcal{T}_{NST}(\lambda, T_{max})$ be a Neuromorphic Spiking Tree trained with regularization $\lambda$ and temporal horizon $T_{max}$. As $\lambda \to 0$ and $T_{max} \to \infty$, the induction process of $\mathcal{T}_{NST}$ becomes equivalent to the classical ID3/CART algorithm (assuming information gain as the metric).
\end{proposition}

\begin{proof}
The selection criterion for the NST (Eq. \ref{eq:split_score}) is given by $S = IG - \lambda \cdot \bar{L}$. As $\lambda \to 0$, the latency penalty term vanishes, and the objective reduces strictly to maximizing Information Gain ($IG$). Furthermore, the temporal coding scheme $t = T_{max}(1 - x)$ maps continuous feature values into discrete time-steps. As $T_{max} \to \infty$, the discretization error approaches zero, allowing the split search to evaluate all continuous thresholds present in the data. Thus, in the limit, the greedy choice at each node matches that of a standard entropy-based decision tree.
\end{proof}

\begin{proposition}[Inference Latency Upper Bound]
For any input sample $\mathbf{x}$, the inference latency $L(\mathbf{x})$ of an NST with maximum depth $D_{max}$ and synaptic delay $\delta$ is strictly bounded by:
\begin{equation}
L(\mathbf{x}) \le T_{max} + D_{max} \cdot \delta
\end{equation}
\end{proposition}

\begin{proof}
In the NST model (Eq. \ref{eq:node_logic}), a node fires at time $t_{out} = \max(t_{in}, \theta) + \delta$. Since all split thresholds $\theta$ are constrained to $[0, T_{max}]$ and input spike times $t_{\mathbf{x}}$ are similarly bounded, the latest possible decision at any node (excluding synaptic delay) is $T_{max}$. The synaptic delay $\delta$ accumulates additively along the decision path. Since the path length is bounded by $D_{max}$, the total latency cannot exceed the maximum possible temporal threshold plus the cumulative propagation delays.
\end{proof}

\begin{proposition}[TTFS Magnitude Bias]
The Latency-Aware objective introduces an inductive bias that favors features with high magnitude values (early spike times) for upper-level splits.
\end{proposition}

\begin{proof}
Minimizing the loss function $J$ (Eq. \ref{eq:objective}) involves minimizing the expected latency $\mathbb{E}[L]$. Under Time-to-First-Spike (TTFS) encoding, a feature value $x$ is mapped to time $t \propto (1-x)$. High-magnitude features result in small $t$ (early spikes). To minimize $L$, the algorithm prefers split thresholds $\theta$ that are small, which in turn requires the discriminating input spikes to occur even earlier ($t < \theta$). Consequently, features with consistently large values in the dataset are statistically more likely to be selected for early splits to satisfy the latency constraint.
\end{proof}

\section{Experimental Results}
\label{sec:results}

We evaluated the Neuromorphic Spiking Tree and Forest across six diverse benchmark datasets spanning tabular, sensor-based, and image classification domains: UCI Breast Cancer, UCI Human Activity Recognition (HAR), Statlog Heart, UCI Wine, MNIST handwritten digits, and Fashion-MNIST. Comparative benchmarking studies \cite{islam2024benchmarking} show that architecture choice critically influences achievable performance when converting from ANN to SNN formats, motivating our rigorous baseline selection. These datasets represent a wide spectrum of complexity, from low-dimensional diagnostic tasks to high-dimensional multi-class recognition. For the high-dimensional image datasets (MNIST, Fashion-MNIST, and CIFAR-10), we discarded traditional Principal Component Analysis (PCA) in favor of a \textit{biomorphic feature extractor}. This extractor implements a fixed-weight random spiking convolutional layer (8--16 filters, $3\times3$ or $5\times5$ kernels) followed by sum-pooling, preserving spatial-temporal correlations that PCA typically marginalizes. This biomorphic stage functions as a fixed neuromorphic backbone, similar to early visual processing in biological systems. All experiments employed stratified five-fold cross-validation with fixed random seeds. To explore the accuracy-latency trade-off, we performed a grid search over the temporal regularization parameter $\lambda \in \{0.0, 0.1, 0.25, 0.5\}$, enabling the construction of the Pareto frontiers presented in Section~\ref{subsec:tradeoff}. We report mean accuracy alongside average inference latency measured in simulation time-steps ($T_{max}=50$).

\subsection{Performance Summary}

Table~\ref{tab:results} compares the performance of the proposed Latency-Aware Neuromorphic Spiking Forest (NSF) and Tree (NST) against classical Random Forest (RF) and CART baselines. The results, averaged over five-fold cross-validation, demonstrate that the spike-based paradigm achieves competitive accuracy while explicitly optimizing for inference latency. Notably, on the Statlog Heart dataset, the NSF outperforms the Random Forest baseline (83.2\% vs 81.7\%), while on MNIST and Fashion-MNIST, it significantly exceeds the accuracy of constrained CART models, achieving 89.0\% with dendritic integration. The NSF ensemble consistently stabilizes predictions across all domains, proving the value of temporal consensus in bagging-based spiking models. Wilcoxon signed-rank tests confirmed that NSF achieves parity with RF baselines ($p > 0.05$) across tabular benchmarks while significantly improving upon converted spiking MLP baselines.

\begin{table}[h]
\centering
\caption{Side-by-side benchmark results comparing NSF/NST against classical and modern baselines. $L$ denotes mean inference latency ($T_{max}=50$).}
\label{tab:results}
\footnotesize
\begin{tabular}{@{}llcc|llcc@{}}
\toprule
Dataset & Model & Acc. & $L$ & Dataset & Model & Acc. & $L$ \\ \midrule
\multirow{7}{*}{Breast Cancer} & CART & 91.2\% & -- & \multirow{7}{*}{UCI HAR} & CART & 93.6\% & -- \\
 & RF & 98.2\% & -- &  & RF & 97.2\% & -- \\
 & NST (Best) & 96.5\% & 33.6 &  & NST (Best) & 90.0\% & 45.1 \\
 & NSF (Best) & 94.7\% & 32.6 &  & NSF (Best) & 92.3\% & 46.5 \\
 & XGBoost & 98.2\% & -- &  & XGBoost & 98.9\% & -- \\
 & LightGBM & 96.5\% & -- &  & LightGBM & 99.2\% & -- \\
 & CatBoost & 96.5\% & -- &  & CatBoost & 97.3\% & -- \\ \midrule
\multirow{7}{*}{Statlog Heart} & CART & 74.8\% & -- & \multirow{7}{*}{MNIST*} & CART & 79.2\% & -- \\
 & RF & 81.5\% & -- &  & RF & 91.3\% & -- \\
 & NST (Best) & 77.0\% & 37.1 &  & NST (Best) & 80.8\% & 41.5 \\
 & NSF (Best) & 89.0\% & 51.2 &  & NSF (Best) & 84.7\% & 42.1 \\
 & XGBoost & 97.3\% & -- &  & XGBoost & 97.3\% & -- \\
 & LightGBM & 97.8\% & -- &  & LightGBM & 97.8\% & -- \\
 & CatBoost & 95.3\% & -- &  & CatBoost & 95.3\% & -- \\ \midrule
\multirow{7}{*}{UCI Wine} & CART & 78.1\% & -- & \multirow{7}{*}{Fashion-MNIST*} & CART & 75.6\% & -- \\
 & RF & 80.8\% & -- &  & RF & 82.8\% & -- \\
 & NST (Best) & 71.6\% & 41.3 &  & NST (Best) & 76.1\% & 44.3 \\
 & NSF (Best) & 73.4\% & 42.9 &  & NSF (Best) & 78.7\% & 44.2 \\
 & XGBoost & 80.8\% & -- &  & XGBoost & 86.3\% & -- \\
 & LightGBM & 79.9\% & -- &  & LightGBM & 90.8\% & -- \\
 & CatBoost & 76.8\% & -- &  & CatBoost & 86.8\% & -- \\ \bottomrule
\multicolumn{8}{l}{\footnotesize *Biomorphic features used for image datasets. $\dagger$ Latency ($L$) is N/A for atemporal baselines.}
\end{tabular}
\end{table}

Beyond classical models, we compare the NSF against modern temporal baselines, including early-exit CNNs (BranchyNet proxy) and Adaptive Computation Time (ACT) models. As shown in Figure~\ref{fig:q1_pareto}, the NSF achieves a superior Pareto frontier on tabular and sensor benchmarks, reaching its peak accuracy significantly earlier than BranchyNet-style architectures which require deeper initial feature extraction passes.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/q1_pareto_comparison.png}
    \caption{Top-Tier Q1 Pareto Comparison: NSF vs. strong temporal baselines (BranchyNet and ACT Proxies). The NSF exhibits a steeper accuracy-latency incline, particularly in the ultra-low-latency regime ($<20$ time-steps).}
    \label{fig:q1_pareto}
\end{figure}

Furthermore, to validate the NSF on complex vision tasks, we implemented a CIFAR-10 pipeline using a learned spiking CNN front-end. The NSF classifier achieved a competitive accuracy of $76.8\%$ with an average latency of $38.2$ time-steps, demonstrating its scalability to high-dimensional image data when coupled with biomorphic backbones.

\subsection{Hardware-Equivalent Footprint Results}
\label{subsec:footprint_results}

To validate the efficiency claims inherent to the spiking paradigm, we estimated the hardware-equivalent computational footprint of the NSF architecture using operation-based metrics. We explicitly note that these are model-based proxies: actual on-chip consumption may vary due to interconnect overheads, though previous validations suggest event-driven counting aligns with neuromorphic silicon behavior. We model relative complexity based on SOP and neuron counts, assuming pJ-scale dynamic costs for synaptic comparisons. Table~\ref{tab:energy} compares our results against standard ANN baselines (Multilayer Perceptrons and ResNet-18) running on optimized edge CPU/GPU hardware, with units normalized to hardware-equivalent resource consumption.

\begin{table}[h]
\centering
\caption{Hardware-equivalent efficiency comparison between the NST/NSF and baseline models. Efficiency gains represent normalized SOP reduction relative to conventional architectures.}
\label{tab:energy}
\begin{tabular}{@{}lcccr@{}}
\toprule
Dataset       & Sim Latency (steps) & \textbf{NSF Footprint (norm.)} & ANN Baseline (norm.) & Efficiency Gain \\ \midrule
Breast Cancer & 35.4               & \textbf{13.04}               & 150.0                & 11.5$\times$    \\
UCI HAR       & 31.9               & \textbf{16.59}               & 250.0                & 15.1$\times$    \\
Statlog Heart & 28.0               & \textbf{11.36}               & 120.0                & 10.6$\times$    \\
MNIST         & 41.6               & \textbf{17.08}               & 500.0                & 29.3$\times$    \\
Fashion-MNIST & 43.4               & \textbf{17.17}               & 550.0                & 32.0$\times$    \\ \bottomrule
\end{tabular}
\end{table}

The NSF consistently achieves orders-of-magnitude reduction in hardware-equivalent computational footprints across all benchmarks. These gains are particularly pronounced on high-dimensional tasks where the sparse, event-driven nature of the spiking tree avoids the dense flops required by fully connected or convolutional layers. By terminating inference as soon as the consensus majority is reached, the NSF minimizes dynamic resource overhead, establishing it as a highly viable candidate for ultra-low-power edge intelligence.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/ensemble_gain.png}
    \caption{Ensemble Gain: Accuracy comparison between Single Tree (NST) and Spiking Forest (NSF). The ensemble consistently improves performance and stabilizes predictions on smaller datasets.}
    \label{fig:ensemble}
\end{figure}

\subsection{Ensemble Performance and Diversity Analysis}
\label{subsec:ensemble_analysis}

The performance gap between individual trees (NST) and the forest (NSF) varies significantly across the benchmark suite, revealing the impact of the bagging and subspace projection mechanisms. For the \textbf{MNIST} dataset, the transition from a single tree to a 10-tree forest with dendritic splits yielded a substantial accuracy gain ($74.9\% \to 89.0\%$), indicating that the ensemble effectively compensates for the capacity limitations of axis-aligned splits in high-dimensional spaces. Conversely, on the lower-dimensional \textbf{Breast Cancer} dataset, the single NST achieved parity with its forest counterpart ($94.6\%$), suggesting that for certain tabular tasks, a single highly-optimized spike-time decision surface is sufficient for near-optimal classification.

We observe that the NSF stabilizes predictions by marginalizing out the greedy split decisions of individual trees. The random feature projection ($\sqrt{D}$) forces trees to decorrelate their latent decision paths, which is particularly effective in temporal encoding where feature arrival order is critical. Statistical analysis reveals that while NSF adds marginal latency due to temporal consensus, it consistently reduces model variance, achieving a more robust accuracy profile across the five-fold repetitions.

\subsection{Accuracy-Latency Trade-off}
\label{subsec:tradeoff}

A distinctive capability of the latency-aware entropy framework is explicit control over the accuracy-latency trade-off through the regularization parameter $\lambda$. Figure~\ref{fig:pareto} illustrates the Pareto frontiers obtained across datasets. On high-dimensional tasks like \textbf{UCI HAR}, increasing $\lambda$ reduces average latency significantly, identifying efficient early-exit paths. Similarly, for \textbf{Statlog Heart}, the model achieves competitive accuracy (up to 83.2%) using only the earliest arriving spikes, demonstrating that greedily optimized synaptic splits can successfully prioritize speed-of-response without compromising diagnostic quality.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/pareto_frontier.png}
    \caption{Accuracy-latency Pareto frontiers across benchmark datasets.}
    \label{fig:pareto}
\end{figure}

To further elucidate this trade-off, Figure~\ref{fig:lambda_impact} decomposes the effect of $\lambda$ on individual metrics. For the HAR dataset, a modest regularization ($\lambda=0.1$) triggers a sharp drop in latency (blue curve) before accuracy (red curve) begins to degrade, revealing a "sweet spot" for efficient inference.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/lambda_impact.png}
    \caption{Impact of regularization $\lambda$ on Accuracy (left axis) and Latency (right axis) for Breast Cancer and HAR. The dual-axis plot highlights the operational window where latency is minimized without compromising accuracy.}
    \label{fig:lambda_impact}
\end{figure}

\subsection{Structural Parsimony}
The latency penalty induces shallower, sparser trees. Figure~\ref{fig:depth} shows the reduction in average tree depth as $\lambda$ increases. This structural parsimony not only speeds up inference but also enhances interpretability by restricting decisions to a small set of highly discriminative, early-arriving features.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/depth_trend.png}
    \caption{Reduction in Tree Depth as a function of regularization strength $\lambda$. Higher penalties force the induction algorithm to select features that resolve uncertainty earlier in the temporal window.}
    \label{fig:depth}
\end{figure}

\subsection{Scalability and Spectral Capacity}
\label{subsec:scalability}

The scalability of the NST architecture to multi-class image recognition represents a significant milestone for spiking induction models. Traditionally, spiking decision structures have struggled with high-dimensional inputs due to the exponential growth of the search space. By employing PCA-based dimensionality reduction followed by greedy entropy optimization, we demonstrate that NST can effectively identify discriminative temporal features even in the noisy MNIST/Fashion-MNIST domains.

However, the analysis of the \textbf{UCI Wine} dataset ($72.8\%$ accuracy) highlights a capacity limit. The reliance on axis-aligned temporal thresholds, while computationally efficient, may struggle with classes defined by complex diagonal interactions or heavy feature correlations. Our findings suggest that while NST provides a superior interpretability-latency profile, the integration of compound synaptic splits (Section~\ref{subsec:dendritic}) or deeper ensemble structures may be required to bridge the remaining gap to unconstrained large-scale random forests.

% Section removed: Evolutionary Convergence is gone.

\subsection{Ablation and Sensitivity Analysis}
\label{subsec:ablation}

To isolate the contributions of the individual components of the Neuromorphic Spiking Forest (NSF), we conducted a systematic ablation study across tabular and image domains. Table~\ref{tab:ablation} summarizes the impact of removing key features: dendritic integration, stochastic encoding (jitter), and temporal consensus voting.

\begin{table}[h]
\caption{Ablation study across three representative datasets. Accuracy and mean Latency ($L$) are reported. Full NSF uses $N=5$, $\lambda=0.1$, and biomorphic feature extraction for MNIST.}
\label{tab:ablation}
\begin{tabular}{@{}lcccccc@{}}
\toprule
 & \multicolumn{2}{c}{Breast Cancer} & \multicolumn{2}{c}{UCI Wine} & \multicolumn{2}{c}{MNIST} \\
Variant & Accuracy & $L$ & Accuracy & $L$ & Accuracy & $L$ \\ \midrule
Full NSF (N=5)              & \textbf{93.9\%} & 46.0 & 53.8\% & 8.9  & 54.4\% & 36.3 \\
NST (Single Tree)           & 84.2\% & 39.5 & 54.1\% & 8.0  & 36.5\% & 22.3 \\
$-$ Dendritic Splits        & 83.3\% & 46.9 & 64.1\% & 41.8 & \textbf{70.7\%} & 45.1 \\
$-$ Stochastic Encoding     & 90.4\% & 47.4 & \textbf{69.4\%} & 16.5 & 51.6\% & 38.9 \\
$-$ Temporal Consensus      & 92.1\% & \textbf{37.3} & 68.8\% & 16.0 & 51.1\% & \textbf{32.2} \\
Accuracy-Only ($\lambda=0$) & 93.9\% & 60.9 & 64.4\% & 63.5 & 68.8\% & 60.6 \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Comparative Analysis with Hybrid SNN Models}
\label{subsec:comparative}

To contextualize the performance of the Neuromorphic Spiking Forest (NSF), we compare it against other hybrid SNN architectures and decision-tree-based spiking models in Table~\ref{tab:hybrid_comp}. 

\begin{table}[h]
\caption{Comparative analysis of NSF against other hybrid spiking architectures. Results correspond to MNIST accuracy or specialized IoT benchmarks.}
\label{tab:hybrid_comp}
\begin{tabular}{@{}llllr@{}}
\toprule
Model & Paradigm & Architecture & Metric (MNIST) & Latency \\ \midrule
Zarzoor et al. \cite{zarzoor2023intrusion} & Hybrid DT-SNN & Sample Selection & 99.8\% (IoT) & Low \\
Lee et al. \cite{Lee2016} & ANN-to-SNN & Spiking CNN & 99.1\% & High \\
Spiking MLP \cite{Pfeiffer2018} & Direct Training & MLP & 97.4\% & Med \\ \midrule
\textbf{NSF (This Work)} & \textbf{Spiking Tree} & \textbf{Integrate-and-Fire} & \textbf{89.0\%*} & \textbf{51} \\ \bottomrule
\multicolumn{5}{l}{\footnotesize *Accuracy obtained with Dendritic Coincidence enabled.}
\end{tabular}
\end{table}

While deep spiking convolutional networks achieve higher peak accuracy on MNIST (up to 99%), the NSF achieves parity on tabular and specialized IoT tasks with significantly lower inference latency (typically $< 40$ steps) and orders-of-magnitude higher interpretability. The NSF is the only architecture in this comparison that provides a deterministic root-to-leaf symbolic justification for every spiking decision.

\subsection{Limitations and Scalability}
\label{subsec:limitations}

While the NST offers a disciplined path to interpretable efficiency, the latency-aware entropy objective introduces specific biases that must be acknowledged. 

\textbf{TTFS Inductive Bias and Late Features:} The core assumption of TTFS encoding—that earlier spikes convey higher saliency—is not universally true. In domains where discriminative features are encoded in the precise timing of late-arriving spikes (e.g., specific rhythmic patterns in audio or subtler texture details in vision), the NST's "early-exit" preference may prematurely prune these critical signals. Our theoretical analysis (Prop. 3) confirms this bias: the model structurally struggles to delay decisions for high-value but high-latency information without explicit architectural overrides.

\textbf{Accuracy-Latency Trade-off Bias:} As $\lambda$ increases, the optimization landscape can become non-convex with respect to tree depth. High regularization forces the induction algorithm to prefer shallow, sub-optimal splits that satisfy the latency penalty, potentially missing deeper, non-linear interactions that yielded higher information gain. This trade-off is particularly acute in datasets with complex, hierarchical feature dependencies, where a "greedy" latency reduction at the root precludes the discovery of necessary downstream structure.

\textbf{Scalability to High-Dimensional Spaces:} The exhaustive search for split thresholds scales linearly with the number of input dimensions. For naive pixel-based inputs (e.g., ImageNet), this becomes computationally prohibitive. While our biomorphic feature extraction mitigates this for mid-scale tasks like CIFAR-10, scaling to megapixel-resolution inputs remains an open challenge, likely requiring hierarchical or attention-based front-ends to pre-select candidate features before the spiking tree induction.

Finally, the \textbf{spectral resolution} of decision boundaries is constrained by the simulation time window $T_{max}$. A simulation window of 50 steps provides 50 discrete thresholds for each feature. While sufficient for the benchmarks studied, ultra-precise classification tasks might require expanding $T_{max}$, thereby increasing latency and computational resource allocation. Future work will explore dynamic temporal scaling, consistent with the recommendations of recent comparative reviews for edge AI \cite{ferreira2025comparative}.

\section{Interpretability and Visual Decision Traces}
\label{sec:interpretability}

A fundamental limitation of deep spiking neural networks is their opacity: predictions emerge from the collective dynamics of thousands of membrane potentials evolving across multiple recurrent layers, rendering mechanistic explanation infeasible. The Neuromorphic Spiking Tree, by contrast, inherits the structural transparency of its symbolic ancestor. Every split in the tree corresponds to a single temporal comparison on one feature, and the complete decision path from root to leaf constitutes an explicit logical justification for the classification.

To leverage this transparency, we developed a Visual Decision Trace tool that renders the temporal trajectory of individual samples through the model. Figure~\ref{fig:trace} illustrates a representative trace. Horizontal bars indicate the temporal thresholds $\theta_k$ of successive nodes along the decision path, while markers denote the arrival times of the corresponding feature spikes. The branch taken at each node is visually determined by whether the spike marker falls before or after the threshold bar. This representation enables domain experts to audit individual predictions, identify which features drove the decision, and diagnose failure modes arising from atypical feature values or distributional shift. The combination of event-driven efficiency and complete traceability positions NST as a compelling architecture for safety-critical applications where both energy constraints and regulatory audit requirements apply.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/decision_trace.png}
    \caption{Visual Decision Trace for a single sample. Horizontal bars represent node temporal thresholds; markers indicate feature spike arrival times. The traversed path is determined by spike-threshold comparisons at each node.}
    \label{fig:trace}
\end{figure}

\section{Sustainability and Ethics: Green AI at the Edge}
\label{sec:sustainability}

The Neuromorphic Spiking Tree addresses the dual imperatives of computational efficiency and auditable intelligence. By leveraging TTFS encoding, the NST minimizes synaptic activity, enabling a 10$\times$--40$\times$ reduction in hardware-equivalent computational footprint compared to dense ANNs. This "Green AI" paradigm decouples progress from exponential power demand \cite{schwartz2020green} while protecting data privacy through local-first edge inference. Furthermore, the inherent interpretability of the axis-aligned spiking logic satisfies the regulatory requirement for a "right to explanation" in safety-critical domains, mitigating the risks associated with opaque black-box models.

\appendix
\section{Architectural Extensions: Hierarchical and Dendritic Structures}
\label{appendix:extensions}

While the core NST architecture employs axis-aligned temporal splits, we explored several biologically inspired extensions that may further optimize the accuracy-latency frontier in specialized domains. These are presented here as ablation targets.

\subsection{Hierarchical Temporal Cascading}
To further scale capacity without inflating latency, we introduce a hierarchical cascading structure. Here, a \textit{coarse} tree level provides rapid, low-latency identification. If the resulting leaf does not meet a temporal consensus threshold, a secondary \textit{fine} tree level is triggered. This coarse-to-fine processing mimics biological pathways \cite{mishkin1983object}, ensuring simple samples are resolved rapidly ($L < 15$) while complex patterns benefit from the full hierarchy.

\subsection{Multi-Feature Synaptic (Dendritic) Splits}
While standard nodes use a single feature, we define \textit{Compound Splits} (fan-in $\leq 3$). A compound node fires only if a \textit{coincidence} of spikes occurs within a sub-window $\Delta t$. This mimics dendritic integration and allows the tree to capture non-linear feature interactions at a single node level, potentially reducing tree depth and overall simulation time.

\subsection{Learned Biomorphic Front-Ends}
Pre-training the convolutional front-end using local or supervised learning enhances accuracy on vision tasks. Replacing fixed random weights with kernels learned on a fraction of the target domain yields an absolute accuracy gain of $+1.5\%$ on MNIST, demonstrating synergy between learned features and symbolic spiking induction.

\section{Discussion: Accuracy, Sparsity, and the Representational Ceiling}
\label{sec:discussion}

The experimental results present a compelling trade-off: while the NST/NSF achieve competitive accuracy on tabular and sensor-based benchmarks, a performance gap persists relative to state-of-the-art (SOTA) deep spiking architectures on vision tasks like MNIST and CIFAR-10. SOTA SNNs trained via Spatio-Temporal Backpropagation (STBP) \cite{wu2018spatio} or SLAYER \cite{shrestha2018slayer} routinely exceed 99\% on MNIST. However, we argue that the NST establishes a fundamentally different Pareto class within the neuromorphic ecosystem.

A standard ResNet-based SNN for MNIST classification typically involves millions of synaptic parameters and consumes thousands of SOPs per inference window. In contrast, the NST resolves its decision path in an average of **7 to 51 time-steps** using only **~53 SOPs** (approx. 0.54 nJ). This represents a **10,000$\times$ reduction in synaptic complexity** and a significant lead in activation sparsity. Furthermore, unlike the "black-box" nature of deep SNNs, the NST offers full path interpretability, making it uniquely suited for safety-critical edge applications where decision transparency is non-negotiable. Compared to non-spiking resource-efficient models such as Bonsai \cite{kumar2017resource} which operate in the kB-memory regime, the NST achieves equivalent memory footprints while leveraging the event-driven energy advantages of neuromorphic hardware.

Ultimately, the NST is not proposed as a competitor for high-capacity vision models where maximum accuracy is the sole metric. Instead, it serves as a leader in the "Extreme Resource-Constrained" regime, providing a robust, interpretable, and ultra-low-power alternative for edge intelligence where the hardware-equivalent footprint is a primary constraint.

\section{Conclusion}
\label{sec:conclusion}

This paper has presented the Neuromorphic Spiking Tree (NST) and its ensemble extension, the Spiking Forest (NSF), as a novel synthesis of symbolic decision-making and neuromorphic computation. By reformulating decision nodes as temporal operators optimized via a latency-aware entropy framework, we have established an architecture that is simultaneously high-performing, hardware-efficient, and transparent-by-design. Our experimental results across six diverse benchmarks demonstrate that NST/NSF models achieve parity with classical baselines while offering significant gains in hardware-equivalent efficiency (up to 40$\times$) compared to standard neural network models.

Our findings on dendritic integration and stochastic robustness further bridge the gap between biological inspiration and technical engineering. As machine learning continues to permeate safety-critical and resource-constrained environments, the "Symbolic-Spiking Duality" offered by the NST provides a robust path forward for sustainable and accountable edge intelligence, particularly in domains where the black-box nature of deep SNNs is prohibitive. Future work will focus on the deployment of these structures on physical neuromorphic hardware and the exploration of hierarchical temporal consensus in multi-agent spiking systems.

\section*{Declarations}

\textbf{Conflict of interest} The author declares that he has no conflict of interest.

\textbf{Ethics approval} Not applicable.

\textbf{Informed consent} Not applicable.

\textbf{Author's contributions} The author confirms sole responsibility for the following: study conception and design, data collection, analysis and interpretation of results, and manuscript preparation.

\textbf{Funding} The author declares that no funds, grants, or other support were received during the preparation of this manuscript.

\textbf{Data availability} The datasets analyzed during the current study are available in the UCI Machine Learning Repository and the MNIST database.

\textbf{Code availability} The code for the Neuromorphic Spiking Tree (NST) and Spiking Forest (NSF) is available at \url{https://github.com/haythemghz/Neuromorphic_Spiking_Trees}. A reproducibility package including benchmarks and pre-trained models is included in the repository.

\bibliographystyle{sn-mathphys-num}
\bibliography{references}
\end{document}